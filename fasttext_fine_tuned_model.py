# -*- coding: utf-8 -*-
"""FastText Fine-tuned model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hfctZEzYKjTUociR_PCa-w1IKiy4cA9d
"""

# Step 1: Install FastText library
!pip install fasttext

# Step 2: Import necessary libraries
import pandas as pd
import nltk
import fasttext
from sklearn.model_selection import train_test_split

nltk.download('punkt')

# Step 3: Upload the dataset
from google.colab import files
uploaded = files.upload()

# Step 4: Load the dataset
df = pd.read_csv(next(iter(uploaded)))

# Step 5: Preprocess the data (tokenize the text)
def preprocess(text):
    return ' '.join(nltk.word_tokenize(text.lower()))

df['content'] = df['content'].apply(preprocess)

# Step 6: Map labels and prepare data in FastText format
df['fasttext_label'] = '__label__' + df['RequirementType'].astype(str)

# Step 7: Split the dataset into training, validation, and testing sets
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
train_df, valid_df = train_test_split(train_df, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2

# Step 8: Save the splits to text files in FastText format
train_df[['fasttext_label', 'content']].to_csv('train_data.txt', index=False, sep=' ', header=False)
valid_df[['fasttext_label', 'content']].to_csv('valid_data.txt', index=False, sep=' ', header=False)
test_df[['fasttext_label', 'content']].to_csv('test_data.txt', index=False, sep=' ', header=False)

# Step 9: Train the FastText model using the training data
model = fasttext.train_supervised(input="train_data.txt", epoch=25, lr=1.0, wordNgrams=2)

# Step 10: Evaluate the model on the validation set
valid_result = model.test("valid_data.txt")
print(f"Validation Accuracy: {valid_result[1] * 100:.2f}%")

# Step 11: Evaluate the model on the test set
test_result = model.test("test_data.txt")
print(f"Test Accuracy: {test_result[1] * 100:.2f}%")

# Step 1: Train and Save the Doc2Vec Model
model.save_model("fasttext_model.bin")

# Step 2: Install huggingface_hub
!pip install huggingface_hub

# Step 3: Login to Hugging Face
from huggingface_hub import notebook_login
notebook_login()

# Step 4: Upload the model to Hugging Face
from huggingface_hub import HfApi

# Initialize Hugging Face API
api = HfApi()

# Upload the model to your Hugging Face repository
api.upload_folder(
    folder_path='./',  # Folder path where 'doc2vec_model' is located
    repo_id='RafidMehda/fasttext_model',  # Your Hugging Face repository name
    repo_type='model'  # Specify that it's a model repository
)