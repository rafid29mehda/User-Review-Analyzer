# -*- coding: utf-8 -*-
"""Doc2vec fine-tune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PvNqusMtk_ZWZDJ1HrcMrFtxQFIBsYTN
"""

!pip install gensim

import pandas as pd
from gensim.models.doc2vec import TaggedDocument
from nltk.tokenize import word_tokenize
import nltk
from google.colab import files

# Step 1: Upload the CSV file in Google Colab
uploaded = files.upload()  # Opens a file dialog for file upload

# Step 2: Load the dataset into a DataFrame
df = pd.read_csv(next(iter(uploaded)))  # Load the uploaded file into a DataFrame

# Step 3: Download NLTK resources
nltk.download('punkt')

# Step 4: Map labels to integers (Functional: 1, Non-Functional: 0)
label_mapping = {'F': 1, 'NF': 0}
df['labels'] = df['RequirementType'].map(label_mapping)

# Step 5: Prepare tagged documents for Doc2Vec
tagged_data = [TaggedDocument(words=word_tokenize(doc.lower()), tags=[str(i)]) for i, doc in enumerate(df['content'])]

from gensim.models import Doc2Vec

# Initialize the Doc2Vec model
model = Doc2Vec(vector_size=100, window=5, min_count=2, workers=4, epochs=20)

# Build the vocabulary from the tagged documents
model.build_vocab(tagged_data)

# Train the model
model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)

# Extract document vectors
doc_vectors = [model.dv[str(i)] for i in range(len(tagged_data))]

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(doc_vectors, df['labels'], test_size=0.2, random_state=42)

# Train a logistic regression classifier
classifier = LogisticRegression()
classifier.fit(X_train, y_train)

# Make predictions
y_pred = classifier.predict(X_test)

# Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy * 100:.2f}%")

# Get precision, recall, F1-score, and support
print("Classification Report:\n")
print(classification_report(y_test, y_pred, target_names=['Non-Functional', 'Functional']))

# Step 1: Train and Save the Doc2Vec Model
model.save('doc2vec_model')

# Step 2: Install huggingface_hub
!pip install huggingface_hub

# Step 3: Login to Hugging Face
from huggingface_hub import notebook_login
notebook_login()

# Step 4: Upload the model to Hugging Face
from huggingface_hub import HfApi

# Initialize Hugging Face API
api = HfApi()

# Upload the model to your Hugging Face repository
api.upload_folder(
    folder_path='./',  # Folder path where 'doc2vec_model' is located
    repo_id='RafidMehda/doc2vec_model',  # Your Hugging Face repository name
    repo_type='model'  # Specify that it's a model repository
)

