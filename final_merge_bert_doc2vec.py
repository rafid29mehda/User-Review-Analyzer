# -*- coding: utf-8 -*-
"""Final Merge Bert Doc2vec.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wANbOYs6TurVMctMkmsTzyw1Q_TsmXh_
"""

!pip install gensim transformers torch scikit-learn

import pandas as pd
from google.colab import files

# Upload the CSV file
uploaded = files.upload()

# Load the dataset into a DataFrame
df = pd.read_csv(next(iter(uploaded)))  # Assumes the first uploaded file is your dataset

# Map 'RequirementType' to 'labels' (Functional: 1, Non-Functional: 0)
label_mapping = {'F': 1, 'NF': 0}
df['labels'] = df['RequirementType'].map(label_mapping)

# Check if the 'labels' column was created correctly
print(df[['RequirementType', 'labels']].head())

from huggingface_hub import hf_hub_download
from gensim.models import Doc2Vec

# Step 5: Download the model file from Hugging Face
model_path = hf_hub_download(repo_id="RafidMehda/doc2vec_model", filename="doc2vec_model")

# Step 6: Load the fine-tuned Doc2Vec model
doc2vec_model = Doc2Vec.load(model_path)

# Extract Doc2Vec embeddings for each document in the dataset
doc2vec_embeddings = [doc2vec_model.dv[str(i)] for i in range(len(df))]

from transformers import DistilBertTokenizer, DistilBertModel
import torch

# Step 8: Load DistilBERT tokenizer and model
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased')

def get_distilbert_embeddings(text):
    # Tokenize the text
    inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)

    # Get DistilBERT embeddings
    with torch.no_grad():
        outputs = distilbert_model(**inputs)
        last_hidden_state = outputs.last_hidden_state
        pooled_embedding = torch.mean(last_hidden_state, dim=1)  # Average pooling of token embeddings
    return pooled_embedding.squeeze().numpy()

# Step 9: Generate DistilBERT embeddings for the dataset
distilbert_embeddings = [get_distilbert_embeddings(doc) for doc in df['content']]

from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import torch
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import accuracy_score, classification_report

# Load the fine-tuned tokenizer and model from Hugging Face
tokenizer = DistilBertTokenizer.from_pretrained('RafidMehda/app_review_model')
model = DistilBertForSequenceClassification.from_pretrained('RafidMehda/app_review_model')

# Ensure the model is on GPU if available
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

# Ensure the model is in evaluation mode
model.eval()

# Assuming 'content' is the column that holds the text in your DataFrame
X_train, X_test, y_train, y_test = train_test_split(df['content'], df['labels'], test_size=0.2, random_state=42)

# Check if X_train and X_test are lists of strings
print(X_train[:5])  # Should print some raw text
print(X_test[:5])

# Tokenize the text data from X_train and X_test with reduced max_length
train_inputs = tokenizer(list(X_train), return_tensors="pt", padding=True, truncation=True, max_length=256)
test_inputs = tokenizer(list(X_test), return_tensors="pt", padding=True, truncation=True, max_length=256)

# Move the tokenized inputs to the correct device (GPU/CPU)
test_inputs = {key: val.to(device) for key, val in test_inputs.items()}

# Define a DataLoader for batching to avoid memory issues
batch_size = 8  # Reduced batch size to avoid memory issues
test_dataset = TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'])
test_loader = DataLoader(test_dataset, batch_size=batch_size)

predicted_classes = []

# Model evaluation in batches
with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask = batch
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        preds = torch.argmax(logits, dim=1)
        predicted_classes.append(preds)

# Concatenate all batch predictions into a single tensor
predicted_classes = torch.cat(predicted_classes)

# Convert the predicted classes to CPU if needed (for compatibility with sklearn)
preds = predicted_classes.cpu().numpy()

# Evaluate the model's performance
accuracy = accuracy_score(y_test, preds)
print(f"Model Accuracy: {accuracy * 100:.2f}%")

# Print detailed classification report
print("Classification Report:\n")
print(classification_report(y_test, preds, target_names=['Non-Functional', 'Functional']))

# Save the fine-tuned model and tokenizer locally
model.save_pretrained('./merge_doc2vec_distilbert_model')
tokenizer.save_pretrained('./merge_doc2vec_distilbert_model')

!pip install huggingface_hub
from huggingface_hub import notebook_login

# Login to your Hugging Face account
notebook_login()

from huggingface_hub import HfApi

# Upload the model to Hugging Face
api = HfApi()
api.upload_folder(
    folder_path='./merge_doc2vec_distilbert_model',  # Path to your fine-tuned model folder
    repo_id='RafidMehda/merge_doc2vec_distilbert_model',  # Your Hugging Face repository
    repo_type='model'
)