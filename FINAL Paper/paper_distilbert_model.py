# -*- coding: utf-8 -*-
"""paper_distilBERT_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FEmfGgzSRgv94LuuQpz-ooOFGTCby17W
"""

# Install necessary libraries
!pip install datasets transformers scikit-learn

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from datasets import Dataset
from google.colab import files
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, DistilBertConfig
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Upload the CSV file
uploaded = files.upload()

# Load the uploaded file into a DataFrame
df = pd.read_csv(next(iter(uploaded)))

# Map labels to integers (Functional: 1, Non-Functional: 0)
label_mapping = {'F': 1, 'NF': 0}
df['labels'] = df['RequirementType'].map(label_mapping)

# Split the dataset into training, validation, and testing sets (60-20-20 split)
train_df, temp_df = train_test_split(df[['content', 'labels']], test_size=0.4, random_state=42)
val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)

# Convert DataFrames to Hugging Face Dataset format
train_dataset = Dataset.from_pandas(train_df)
val_dataset = Dataset.from_pandas(val_df)
test_dataset = Dataset.from_pandas(test_df)

# Load pre-trained DistilBERT tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Tokenization function
def tokenize_function(examples):
    return tokenizer(examples['content'], padding="max_length", truncation=True)

# Tokenize datasets
train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

# Modify dropout in the DistilBERT configuration
config = DistilBertConfig.from_pretrained('distilbert-base-uncased', num_labels=2, dropout=0.3, attention_dropout=0.3)

# Load DistilBERT model with modified configuration for sequence classification
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', config=config)

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy="epoch",
    learning_rate=1e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=2,
    weight_decay=0.01,
    logging_dir='./logs',
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

# Fine-tune the model
trainer.train()

# Evaluation function for custom outputs
def evaluate_and_print_results(dataset, dataset_name):
    predictions = trainer.predict(dataset)
    preds = np.argmax(predictions.predictions, axis=-1)
    labels = dataset['labels']

    accuracy = accuracy_score(labels, preds)
    precision = precision_score(labels, preds, average='weighted')
    recall = recall_score(labels, preds, average='weighted')
    f1 = f1_score(labels, preds, average='weighted')

    print(f"{dataset_name} Set Classification Report:\n")
    print(classification_report(labels, preds, target_names=['Non-Functional', 'Functional'], zero_division=1))
    print(f"{dataset_name} Set Accuracy: {accuracy * 100:.2f}%\n")

# Evaluate on Training Set
evaluate_and_print_results(train_dataset, "Training")

# Evaluate on Validation Set
evaluate_and_print_results(val_dataset, "Validation")

# Evaluate on Test Set
evaluate_and_print_results(test_dataset, "Test")

# Save the fine-tuned model
trainer.save_model('./paper_fined-distilBERT')

# Save the tokenizer files
tokenizer.save_pretrained('./paper_fined-distilBERT')

!pip install huggingface_hub

from huggingface_hub import notebook_login

notebook_login()

from huggingface_hub import HfApi

# Upload the entire directory to Hugging Face
api = HfApi()
api.upload_folder(
    folder_path='./paper_fined-distilBERT',  # Path to the folder with the model and tokenizer files
    repo_id='RafidMehda/paper_fined-distilBERT',  # Your model repository on Hugging Face
    repo_type='model'
)