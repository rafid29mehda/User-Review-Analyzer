# -*- coding: utf-8 -*-
"""Label_Reviews.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/rafid29mehda/User-Review-Analyzer/blob/main/Label_Reviews.ipynb

Step 1
"""

# Install required packages
!pip install transformers
!pip install datasets
!pip install torch
!pip install pandas
!pip install scikit-learn  
!pip install tqdm
!pip install pyarrow==14.0.1

"""Step 2

"""

import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from datasets import Dataset
from tqdm import tqdm

"""Step 3, 4"""

from google.colab import files
from transformers import BertTokenizer, BertForSequenceClassification
from datasets import Dataset

# Load PROMISE_exp1.csv from your Google Drive
uploaded = files.upload()

# Load the PROMISE dataset
promise_df = pd.read_csv('PROMISE_exp1.csv')

# Map labels to integers (F -> 0, NF -> 1)
promise_df['_class_'] = promise_df['_class_'].map({'F': 0, 'NF': 1})

# Split the data into training and validation sets
train_df, val_df = train_test_split(promise_df, test_size=0.2, random_state=42)

# Create Dataset objects
train_dataset = Dataset.from_pandas(train_df[['RequirementText', '_class_']])
val_dataset = Dataset.from_pandas(val_df[['RequirementText', '_class_']])

# Load BERT tokenizer and model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Tokenize the dataset and rename '_class_' to 'labels'
def tokenize_function(examples):
    return tokenizer(examples['RequirementText'], padding="max_length", truncation=True)

# Apply the tokenizer to the dataset
train_dataset = train_dataset.map(tokenize_function, batched=True)
val_dataset = val_dataset.map(tokenize_function, batched=True)

# Rename '_class_' to 'labels' to make it compatible with Trainer
train_dataset = train_dataset.rename_column('_class_', 'labels')
val_dataset = val_dataset.rename_column('_class_', 'labels')

# Set the format for PyTorch
train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

"""Step 5"""

# Training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    eval_strategy="epoch", 
)

# Define the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

# Fine-tune the model
trainer.train()

"""Step 6"""

# Save the model and tokenizer
model.save_pretrained('fine-tuned-bert')
tokenizer.save_pretrained('fine-tuned-bert')

"""Step 7"""

# Load reviews.csv from your Google Drive
uploaded = files.upload()

# Load the reviews dataset
reviews_df = pd.read_csv('reviews.csv')

# Load the fine-tuned model and tokenizer
model = BertForSequenceClassification.from_pretrained('fine-tuned-bert')
tokenizer = BertTokenizer.from_pretrained('fine-tuned-bert')

# Function to classify reviews
def classify_review(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
    logits = outputs.logits
    predicted_class = torch.argmax(logits, dim=1).item()
    return 'F' if predicted_class == 0 else 'NF'

# Apply classification to the reviews
tqdm.pandas()
reviews_df['RequirementType'] = reviews_df['content'].progress_apply(classify_review)

# Save the labeled reviews
reviews_df.to_csv('labeled_reviews.csv', index=False)
print("Labeled reviews saved to 'labeled_reviews.csv'.")

"""Step 8"""

from google.colab import files
files.download('labeled_reviews.csv')

