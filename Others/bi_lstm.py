# -*- coding: utf-8 -*-
"""Bi-LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FEN6rX_S-f1RX16wJDfe6szhdNSE9T2_
"""

!pip install gensim transformers torch scikit-learn tqdm nltk
import pandas as pd
import numpy as np
import nltk
from nltk.tokenize import sent_tokenize
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from gensim.models import Doc2Vec
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm
from huggingface_hub import hf_hub_download
from google.colab import files

# Upload the file
uploaded = files.upload()

# Load the uploaded CSV into a DataFrame
df = pd.read_csv(next(iter(uploaded)))

# Map 'RequirementType' to 'labels' (Functional: 1, Non-Functional: 0)
label_mapping = {'F': 1, 'NF': 0}
df['labels'] = df['RequirementType'].map(label_mapping)

# Tokenize documents into sentences
nltk.download('punkt')
df['sentences'] = df['content'].apply(sent_tokenize)

# Download and load the Doc2Vec model from Hugging Face
model_path = hf_hub_download(repo_id="RafidMehda/doc2vec_model", filename="doc2vec_model")
doc2vec_model = Doc2Vec.load(model_path)

# Load pre-trained DistilBERT model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("RafidMehda/fined-distilBERT")
distilbert_model = AutoModel.from_pretrained("RafidMehda/fined-distilBERT")

# Function to get Doc2Vec embeddings
def get_doc2vec_embeddings(index):
    doc2vec_emb = doc2vec_model.dv[str(index)]
    return doc2vec_emb

# Apply the function to extract Doc2Vec embeddings
doc2vec_embeddings = [get_doc2vec_embeddings(i) for i in range(len(df))]

# Function to get DistilBERT embeddings for sentences in a document
def get_distilbert_sentence_embeddings(sentences):
    sentence_embeddings = []
    for sentence in sentences:
        inputs = tokenizer(sentence, return_tensors="pt", padding=True, truncation=True, max_length=128)
        with torch.no_grad():
            outputs = distilbert_model(**inputs)
            last_hidden_state = outputs.last_hidden_state
            pooled_embedding = torch.mean(last_hidden_state, dim=1)
        sentence_embeddings.append(pooled_embedding.squeeze().numpy())
    return sentence_embeddings

# Apply the function to get DistilBERT embeddings for each document's sentences
df['sentence_embeddings'] = df['sentences'].apply(get_distilbert_sentence_embeddings)

class Attention(nn.Module):
    def __init__(self, hidden_dim):
        super(Attention, self).__init__()
        self.attn_weights = nn.Parameter(torch.Tensor(hidden_dim, 1))
        nn.init.xavier_uniform_(self.attn_weights)

    def forward(self, hidden_states):
        attn_scores = torch.tanh(torch.matmul(hidden_states, self.attn_weights)).squeeze(-1)
        attn_weights = F.softmax(attn_scores, dim=1)
        weighted_sum = torch.bmm(attn_weights.unsqueeze(1), hidden_states).squeeze(1)
        return weighted_sum, attn_weights

# Initialize the attention model
sentence_attention = Attention(hidden_dim=768)

# Apply attention mechanism
sentence_attention_outputs = []
for emb_list in df['sentence_embeddings']:
    sentence_embs = torch.tensor(np.array(emb_list))
    if len(sentence_embs.shape) == 2:
        sentence_embs = sentence_embs.unsqueeze(0)
    weighted_sum, _ = sentence_attention(sentence_embs)
    sentence_attention_outputs.append(weighted_sum.detach().numpy())

combined_embeddings = []
for doc2vec_emb, sentence_attn_emb in zip(doc2vec_embeddings, sentence_attention_outputs):
    sentence_attn_emb_flat = sentence_attn_emb.flatten()
    combined_embedding = np.concatenate((doc2vec_emb, sentence_attn_emb_flat))
    combined_embeddings.append(combined_embedding)

X = np.array(combined_embeddings)
y = df['labels'].values

# Split data into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Define an adjusted Bi-LSTM model with two layers and reduced dropout
class AdjustedBiLSTMClassifier(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, dropout_prob=0.1):
        super(AdjustedBiLSTMClassifier, self).__init__()
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=2, bidirectional=True, batch_first=True, dropout=dropout_prob)
        self.fc = nn.Linear(hidden_dim * 2, output_dim)
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        lstm_out = lstm_out[:, 0, :]  # Only use the first time step
        logits = self.fc(lstm_out)
        return self.softmax(logits)

# Initialize model with weight decay for L2 regularization and no dropout
input_dim = X_train.shape[1]
hidden_dim = 128
output_dim = 2
model = AdjustedBiLSTMClassifier(input_dim, hidden_dim, output_dim, dropout_prob=0.1)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)  # L2 regularization

# Training loop with early stopping
best_val_accuracy = 0
patience = 5
wait = 0
epochs = 50
for epoch in range(epochs):
    model.train()
    inputs = torch.tensor(X_train, dtype=torch.float32).unsqueeze(1)  # Add sequence dimension
    labels = torch.tensor(y_train, dtype=torch.long)
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()

    # Validation step
    model.eval()
    with torch.no_grad():
        val_inputs = torch.tensor(X_val, dtype=torch.float32).unsqueeze(1)
        val_labels = torch.tensor(y_val, dtype=torch.long)
        val_outputs = model(val_inputs)
        _, val_predicted = torch.max(val_outputs, 1)
        val_accuracy = accuracy_score(val_labels.numpy(), val_predicted.numpy())

    # Early stopping check
    if val_accuracy > best_val_accuracy:
        best_val_accuracy = val_accuracy
        wait = 0
        torch.save(model.state_dict(), 'best_model.pt')
    else:
        wait += 1
        if wait >= patience:
            print(f"Early stopping at epoch {epoch + 1}")
            break

    print(f"Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}, Validation Accuracy: {val_accuracy * 100:.2f}%")

# Load the best model for final evaluation
model.load_state_dict(torch.load('best_model.pt'))

# Evaluate the model on the test set
evaluate(X_train, y_train, "Training Set")
evaluate(X_val, y_val, "Validation Set")
evaluate(X_test, y_test, "Test Set")