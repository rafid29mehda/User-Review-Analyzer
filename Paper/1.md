
---

# **Title**

**A Hybrid NLP Model Integrating Document-Level and Contextualized Word-Level Embeddings for User Review Classification**

# **Abstract**

*(A concise summary of the paper, typically 150-250 words.)*

**Abstract:**  
We present a hybrid natural language processing (NLP) model that combines document-level and contextualized word-level embeddings to classify user reviews into Functional (F) and Non-Functional (NF) categories. Leveraging fine-tuned Doc2Vec and DistilBERT models, we generate comprehensive embeddings that capture both global and contextual semantics. Our approach significantly improves classification accuracy, achieving a 97.55% accuracy rate on the test set, outperforming individual models. Extensive experiments demonstrate the effectiveness of the hybrid model in handling nuanced language in user reviews.

# **Keywords**

Natural Language Processing, Doc2Vec, DistilBERT, Embeddings, User Review Classification, Functional Requirements, Non-Functional Requirements, Hybrid Model, XGBoost

# **1. Introduction**

*(Introduce the problem, its significance, and your contribution.)*

The classification of user reviews into Functional (F) and Non-Functional (NF) requirements is a critical task in software engineering, aiding developers in understanding user needs and improving product features. Traditional models often struggle to capture the nuanced language present in user reviews, leading to suboptimal classification performance.

In this paper, we introduce a hybrid NLP model that integrates document-level embeddings from Doc2Vec and contextualized word-level embeddings from DistilBERT. By combining these embeddings, we aim to leverage the strengths of both global semantic understanding and contextual nuance. We fine-tune both models on a domain-specific dataset and use XGBoost as the classifier to exploit complex relationships within the data.

Our hybrid model achieves a remarkable accuracy of 97.55%, significantly outperforming the individual models. This work demonstrates the efficacy of combining different embedding techniques for enhanced text classification tasks.

# **2. Related Work**

*(Discuss previous research and how your work fits in.)*

Text classification has been a fundamental task in NLP, with various approaches ranging from traditional machine learning algorithms to deep learning models. Doc2Vec [1] provides document-level embeddings capturing the global semantics of texts, while models like BERT [2] and its distilled version, DistilBERT [3], offer contextualized word-level embeddings that have set new benchmarks in various NLP tasks.

Recent studies have explored combining multiple embedding methods to improve classification performance [4][5]. However, few have specifically addressed the classification of user reviews into F and NF categories using a hybrid approach.

Our work builds upon these foundations by fine-tuning Doc2Vec and DistilBERT on a domain-specific dataset and integrating their embeddings. This hybrid approach addresses the limitations of individual models and enhances the model's ability to understand both the overall context and specific nuances in user reviews.

# **3. Methodology**

*(Detailed explanation of your approach.)*

## **3.1. Dataset Preparation**

We utilize two datasets:

- **PROMISE_exp.csv**: A publicly available dataset containing labeled requirements as Functional (F) or Non-Functional (NF).
- **reviews.csv**: An unlabeled dataset of user reviews sourced from the Hugging Face dataset collection.

### **3.1.1. Labeling the Unlabeled Dataset**

To label the `reviews.csv` dataset:

1. **Fine-tune BERT on PROMISE_exp.csv**: We map the labels 'F' and 'NF' to integers and split the dataset into training and validation sets. Using the Hugging Face `Trainer`, we fine-tune a BERT model for sequence classification.
   
2. **Label reviews.csv**: The fine-tuned BERT model is then used to classify each review in `reviews.csv` as F or NF, resulting in the `labeled_reviews.csv` dataset.

## **3.2. Model Training**

### **3.2.1. Fine-tuning Doc2Vec**

1. **Data Preparation**: We tokenize the text data and create `TaggedDocument` objects required for training Doc2Vec.
   
2. **Model Training**: A Doc2Vec model is initialized with specific hyperparameters and trained on the tagged data.
   
3. **Feature Extraction**: Document embeddings are extracted for each review.

### **3.2.2. Fine-tuning DistilBERT**

1. **Data Preparation**: The labeled dataset is split into training and test sets. The text data is tokenized using the DistilBERT tokenizer.
   
2. **Model Training**: We fine-tune a DistilBERT model for sequence classification using the Hugging Face `Trainer`.
   
3. **Feature Extraction**: Contextualized embeddings are obtained by averaging the token embeddings from the last hidden state of the model.

## **3.3. Hybrid Model Construction**

1. **Embedding Combination**: We concatenate the Doc2Vec embeddings (document-level) with the DistilBERT embeddings (word-level) for each review.
   
2. **Classification with XGBoost**: The combined embeddings serve as input features to an XGBoost classifier, which is trained to predict the F or NF labels.

# **4. Experiments and Results**

*(Present your findings with tables and charts.)*

## **4.1. Experimental Setup**

- **Environment**: All experiments are conducted using Python, with libraries such as Transformers, Gensim, Scikit-learn, and XGBoost.
- **Evaluation Metrics**: We use accuracy, precision, recall, and F1-score to evaluate model performance.

## **4.2. Individual Model Performance**

### **4.2.1. Doc2Vec Model**

- **Accuracy**: 73.91%
- **Classification Report**:

  | Class            | Precision | Recall | F1-Score | Support |
  |------------------|-----------|--------|----------|---------|
  | Non-Functional   | 74%       | 82%    | 77%      | 1366    |
  | Functional       | 74%       | 65%    | 69%      | 1133    |

- **Analysis**: The Doc2Vec model shows moderate performance, with a higher recall for Non-Functional requirements. This indicates a tendency to better identify Non-Functional reviews but also suggests potential false negatives for Functional reviews.

### **4.2.2. DistilBERT Model**

- **Accuracy**: 93.17%
- **Classification Report**:

  | Class            | Precision | Recall | F1-Score | Support |
  |------------------|-----------|--------|----------|---------|
  | Non-Functional   | 99%       | 88%    | 93%      | 2081    |
  | Functional       | 87%       | 99%    | 93%      | 1668    |

- **Analysis**: The DistilBERT model significantly outperforms the Doc2Vec model, thanks to its ability to capture contextual nuances. However, there's a slight imbalance in precision and recall between classes.

## **4.3. Hybrid Model Performance**

- **Accuracy**: 97.55%
- **Classification Report**:

  | Class            | Precision | Recall | F1-Score | Support |
  |------------------|-----------|--------|----------|---------|
  | Non-Functional   | 98%       | 98%    | 98%      | 1036    |
  | Functional       | 98%       | 97%    | 97%      | 839     |

- **Analysis**: The hybrid model achieves superior performance, indicating that combining embeddings enhances the model's ability to classify reviews accurately. The high precision and recall across both classes demonstrate balanced performance.

## **4.4. Visualization of Results**

*(Include charts and graphs for better visualization.)*

### **4.4.1. Accuracy Comparison**

![Accuracy Comparison](attachment:accuracy_comparison.png)

*Figure 1: Comparison of accuracy across models.*

### **4.4.2. Precision, Recall, and F1-Score**

![Metrics Comparison](attachment:metrics_comparison.png)

*Figure 2: Precision, Recall, and F1-Score for each model.*

### **4.4.3. Confusion Matrices**

**Doc2Vec Model Confusion Matrix**

![Doc2Vec Confusion Matrix](attachment:doc2vec_confusion_matrix.png)

*Figure 3: Confusion matrix for the Doc2Vec model.*

**DistilBERT Model Confusion Matrix**

![DistilBERT Confusion Matrix](attachment:distilbert_confusion_matrix.png)

*Figure 4: Confusion matrix for the DistilBERT model.*

**Hybrid Model Confusion Matrix**

![Hybrid Model Confusion Matrix](attachment:hybrid_confusion_matrix.png)

*Figure 5: Confusion matrix for the hybrid model.*

# **5. Discussion**

*(Interpret the results, discuss the implications and any limitations.)*

The experimental results demonstrate that the hybrid model significantly outperforms the individual Doc2Vec and DistilBERT models. By combining document-level and contextualized word-level embeddings, the model effectively captures both the global semantic context and the fine-grained nuances in user reviews.

The high accuracy and balanced precision and recall indicate that the model is robust and generalizes well to unseen data. The use of XGBoost as the classifier leverages its capability to model complex relationships, further enhancing performance.

**Limitations**:

- **Computational Complexity**: Combining embeddings increases the dimensionality of the input features, potentially leading to higher computational costs.
- **Data Imbalance**: Although mitigated, any residual imbalance in the dataset could affect model performance.

# **6. Conclusion**

*(Summarize your findings and suggest future work.)*

We have presented a hybrid NLP model that integrates Doc2Vec and DistilBERT embeddings for classifying user reviews into Functional and Non-Functional categories. The model achieves a high accuracy of 97.55%, outperforming individual models.

This work demonstrates the effectiveness of combining different embedding techniques to enhance classification tasks. Future work could explore dimensionality reduction techniques to address computational complexity and investigate the model's applicability to other text classification domains.

# **References**

*(List all references cited in the paper.)*

[1] Le, Q., & Mikolov, T. (2014). Distributed Representations of Sentences and Documents. *International Conference on Machine Learning (ICML)*.

[2] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. *NAACL-HLT*.

[3] Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. *arXiv preprint arXiv:1910.01108*.

[4] Qiu, X., Sun, T., Xu, Y., Shao, Y., Dai, N., & Huang, X. (2020). Pre-trained Models for Natural Language Processing: A Survey. *Science China Technological Sciences*, 63(10), 1872â€“1897.

[5] Wang, J., Yu, L., Lai, K., & Zhang, X. (2016). Dimensional Sentiment Analysis Using a Regional CNN-LSTM Model. *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics*.

# **Appendices**

*(Include any additional material, such as detailed code snippets or extended results.)*

---

**Note**: Ensure to replace placeholder text like image paths (e.g., `attachment:accuracy_comparison.png`) with actual figures and tables generated from your experiments. Additionally, expand each section with more detailed explanations, methodologies, and discussions as needed to meet the standards of the target conference.

**Final Remarks**

Writing a comprehensive conference paper involves meticulous attention to detail and thorough explanation of your work. Make sure to:

- **Use Clear Language**: Keep explanations clear and concise.
- **Follow Formatting Guidelines**: Adhere to the conference's submission guidelines regarding formatting, font sizes, and page limits.
- **Cite Properly**: Reference all sources accurately to avoid plagiarism.
- **Proofread**: Review the paper multiple times to correct any grammatical errors or inconsistencies.


