Here's a detailed structure for your conference paper. This format adheres to the standards of scientific papers, and I will provide in-depth explanations and analysis throughout the sections, including tables and charts for better visualization.

---

### Title:  
**A Hybrid NLP Model for Classifying User Reviews into Functional and Non-Functional Categories Using Doc2Vec and DistilBERT**

---

### Abstract:  
We propose a hybrid natural language processing (NLP) model that integrates document-level and contextualized word-level embeddings to classify user reviews into functional (F) and non-functional (NF) categories. By combining the strengths of two pre-trained models, Doc2Vec and DistilBERT, the proposed methodology demonstrates significant performance improvements over standalone approaches. The model is fine-tuned on domain-specific user reviews, achieving an overall classification accuracy of 97.55%, significantly surpassing traditional document-level embeddings and individual DistilBERT fine-tuning. Our approach effectively addresses the complexities of user reviews by capturing both high-level document features and granular word-level contexts, making it a robust solution for real-world application in review classification.

---

### 1. **Introduction**

The increasing volume of user-generated reviews on digital platforms has led to a demand for automated systems that can classify and analyze these reviews accurately. Classifying reviews as functional (F) or non-functional (NF) is critical in industries such as software development, where identifying functional requirements can streamline project planning and decision-making. Traditional approaches to text classification often fail to capture both document-level semantics and word-level contextual nuances effectively.

This paper introduces a hybrid NLP model that combines Doc2Vec's document embeddings with DistilBERT's word-level contextual embeddings. Our approach leverages pre-trained models fine-tuned on domain-specific datasets to achieve significant improvements in review classification accuracy. By integrating both embeddings, we address the limitations of single-model approaches, creating a more nuanced and effective classifier for functional and non-functional reviews.

---

### 2. **Related Work**

Previous research on text classification has explored various approaches, including traditional machine learning algorithms and deep learning models such as Recurrent Neural Networks (RNNs) and Transformer-based models. Doc2Vec [1], a widely used document-level embedding model, has proven effective in capturing semantic representations of entire documents. However, it often struggles with short and context-dependent texts, such as user reviews. On the other hand, Transformer models like BERT [2] and DistilBERT [3] excel at capturing word-level dependencies within a sentence, but they lack the document-level view that Doc2Vec provides.

In our research, we hypothesize that combining both document-level and word-level embeddings can lead to a more comprehensive understanding of user reviews, thereby improving classification accuracy. The hybrid model proposed in this paper builds on these previous works by merging the strengths of both Doc2Vec and DistilBERT.

---

### 3. **Methodology**

Our methodology consists of three main stages: (1) fine-tuning Doc2Vec, (2) fine-tuning DistilBERT, and (3) merging the embeddings from both models into a hybrid classifier using XGBoost.

#### 3.1 **Dataset Preparation**
We start by using the **PROMISE_exp1.csv** dataset, which contains functional and non-functional labels for software requirements. This dataset is used to train both models. We then apply these trained models to classify a separate set of user reviews from **reviews.csv**.

The **PROMISE_exp1.csv** dataset includes two key columns:
- **RequirementText**: Describes functional or non-functional requirements.
- **_class_**: Labels indicating functional (F) or non-functional (NF) requirements.

The **reviews.csv** dataset includes columns such as:
- **Content**: Actual review text.
- **Labels**: Manually labeled as either functional or non-functional after model inference.

---

### 4. **Doc2Vec Fine-Tuning**

We fine-tune a Doc2Vec model using the **reviews.csv** dataset. Each review is tokenized, and its document embedding is extracted. These embeddings are used to classify the review as functional or non-functional. After training, we evaluate the model's performance using standard classification metrics.

#### 4.1 **Results**
The classification report for the fine-tuned Doc2Vec model is shown below:

| Class            | Precision | Recall | F1-Score | Support |
|------------------|-----------|--------|----------|---------|
| Non-Functional   | 0.74      | 0.82   | 0.77     | 1366    |
| Functional       | 0.74      | 0.65   | 0.69     | 1133    |
| **Accuracy**     |           |        | 0.74     | 2499    |
| **Macro Avg**    | 0.74      | 0.73   | 0.73     | 2499    |
| **Weighted Avg** | 0.74      | 0.74   | 0.74     | 2499    |

The overall accuracy of the Doc2Vec model is **73.91%**. While the model performs reasonably well, it struggles with classifying functional requirements, as indicated by its lower recall score for the functional class.

---

### 5. **DistilBERT Fine-Tuning**

Next, we fine-tune the DistilBERT model, using the same labeled review dataset. DistilBERT's contextualized word embeddings enable the model to capture deeper semantic meaning in the reviews, particularly useful for context-heavy sentences.

#### 5.1 **Results**

| Class            | Precision | Recall | F1-Score | Support |
|------------------|-----------|--------|----------|---------|
| Non-Functional   | 0.99      | 0.88   | 0.93     | 2081    |
| Functional       | 0.87      | 0.99   | 0.93     | 1668    |
| **Accuracy**     |           |        | 0.93     | 3749    |
| **Macro Avg**    | 0.93      | 0.94   | 0.93     | 3749    |
| **Weighted Avg** | 0.94      | 0.93   | 0.93     | 3749    |

DistilBERT achieves an accuracy of **93.17%**, significantly outperforming the Doc2Vec model. The precision and recall are balanced for both classes, with an F1-score of **0.93** for both functional and non-functional reviews.

---

### 6. **Hybrid Model: Merging Doc2Vec and DistilBERT Embeddings**

The final stage involves combining the document-level embeddings from Doc2Vec with the word-level embeddings from DistilBERT. We concatenate these embeddings for each review and feed them into an XGBoost classifier for final classification. The intuition behind this approach is that Doc2Vec captures high-level document information, while DistilBERT provides granular word-level insights.

#### 6.1 **Results**

The hybrid model achieves outstanding performance, as shown in the classification report below:

| Class            | Precision | Recall | F1-Score | Support |
|------------------|-----------|--------|----------|---------|
| Non-Functional   | 0.98      | 0.98   | 0.98     | 1036    |
| Functional       | 0.98      | 0.97   | 0.97     | 839     |
| **Accuracy**     |           |        | 0.98     | 1875    |
| **Macro Avg**    | 0.98      | 0.97   | 0.98     | 1875    |
| **Weighted Avg** | 0.98      | 0.98   | 0.98     | 1875    |

The combined model yields an accuracy of **97.55%**, demonstrating a significant improvement over both individual models. The hybrid approach balances the document-level and contextualized word-level information, providing more accurate predictions across both classes.

---

### 7. **Discussion**

The results of the hybrid model show a dramatic improvement in performance, underscoring the benefits of combining multiple embeddings for text classification. Both Doc2Vec and DistilBERT contribute uniquely to the final classification:

- **Doc2Vec** excels in capturing general document-level semantics, but it struggles with the nuances in context-specific sentences typical in user reviews.
- **DistilBERT**, with its transformer-based architecture, handles word-level contextual dependencies efficiently but misses broader document-level information.

By merging both embeddings, our hybrid model captures a holistic view of the review text, resulting in near-perfect accuracy.

#### 7.1 **Comparison of Models**

| Model        | Accuracy | Non-Functional F1-Score | Functional F1-Score |
|--------------|----------|-------------------------|---------------------|
| Doc2Vec      | 73.91%   | 0.77                    | 0.69                |
| DistilBERT   | 93.17%   | 0.93                    | 0.93                |
| Hybrid Model | 97.55%   | 0.98                    | 0.97                |

#### 7.2 **Performance Analysis**

The hybrid model demonstrates improved precision, recall, and F1-scores, particularly in cases where standalone models may struggle due to either lack of document-level or word-level focus. The integration of embeddings enables a more comprehensive representation, which is crucial for real-world tasks involving the classification of user reviews.

---

### 8. **Conclusion**

In this paper, we propose a novel hybrid NLP model that effectively combines Doc2Vec's document-level embeddings with DistilBERT's word-level contextual embeddings. This approach
