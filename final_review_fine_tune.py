# -*- coding: utf-8 -*-
"""Final_Review_Fine_tune.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/rafid29mehda/User-Review-Analyzer/blob/main/Final_Review_Fine_tune.ipynb
"""

!pip install transformers datasets torch

import pandas as pd
from sklearn.model_selection import train_test_split
from datasets import Dataset
from google.colab import files

# Upload the CSV file
uploaded = files.upload()

# Load the uploaded file into a DataFrame
df = pd.read_csv(next(iter(uploaded)))

# Map labels to integers (Functional: 1, Non-Functional: 0)
label_mapping = {'F': 1, 'NF': 0}
df['labels'] = df['RequirementType'].map(label_mapping)

# Split the dataset into train and test
train_df, test_df = train_test_split(df[['content', 'labels']], test_size=0.2, random_state=42)

# Convert to Hugging Face Dataset format
train_dataset = Dataset.from_pandas(train_df)
test_dataset = Dataset.from_pandas(test_df)

from transformers import DistilBertTokenizer

# Load pre-trained DistilBERT tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

def tokenize_function(examples):
    return tokenizer(examples['content'], padding="max_length", truncation=True)

# Tokenize datasets
train_dataset = train_dataset.map(tokenize_function, batched=True)
test_dataset = test_dataset.map(tokenize_function, batched=True)

from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments

# Load DistilBERT model for sequence classification
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir='./logs',
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)

# Fine-tune the model
trainer.train()

import numpy as np
from sklearn.metrics import accuracy_score

# Step 1: Get predictions from the model on the test set
predictions = trainer.predict(test_dataset)

# Step 2: Convert logits to predicted class
preds = np.argmax(predictions.predictions, axis=-1)

# Step 3: Calculate accuracy
accuracy = accuracy_score(test_df['labels'], preds)

# Step 4: Print accuracy
print(f"Model Accuracy: {accuracy * 100:.2f}%")

# Save the fine-tuned model
trainer.save_model('./fin_review_model')

# Save the tokenizer files
tokenizer.save_pretrained('./fin_review_model')

!pip install huggingface_hub

from huggingface_hub import notebook_login

notebook_login()

from huggingface_hub import HfApi

# Upload the entire directory to Hugging Face
api = HfApi()
api.upload_folder(
    folder_path='./fin_review_model',  
    repo_id='RafidMehda/fin_review_model',  
    repo_type='model'
)

