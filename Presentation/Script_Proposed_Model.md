Here’s the combined script for your "Proposed Model Overview" slide:

---

"Building on the identified research gaps, these limitations highlight the need for a hybrid model that addresses these challenges. Our proposed H2AN-BiLSTM model steps in to fill these gaps by combining the strengths of Doc2Vec for capturing document-level semantics and DistilBERT for word-level contextual embeddings. This hybrid approach is further enhanced by a Hierarchical Attention Network (HAN), which focuses on relevant features, creating a robust solution for classifying both functional and non-functional requirements.

This combined embeddings are processed by a Bidirectional LSTM (Bi-LSTM), which examines sequences in both forward and backward directions. This step enhances context understanding by capturing long-range dependencies and sequential information within the text.

Together, these components create a robust and scalable solution for classifying both functional and non-functional requirements. In the next section, I’ll explain how this model was implemented and the results it achieved."

